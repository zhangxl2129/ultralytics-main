<div align="center">
  <p>
    <a href="https://www.ultralytics.com/events/yolovision" target="_blank">
      <img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="YOLO Vision banner"></a>
  </p>
相关数据集下载：通过百度网盘分享的文件：trainingr.zip链接：https://pan.baidu.com/s/1gA3fHWneXgnpWwKnvG75NA?pwd=o0f3 提取码：o0f3

[中文](https://docs.ultralytics.com/zh) | [한국어](https://docs.ultralytics.com/ko) | [日本語](https://docs.ultralytics.com/ja) | [Русский](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [Français](https://docs.ultralytics.com/fr) | [Español](https://docs.ultralytics.com/es) | [Português](https://docs.ultralytics.com/pt) | [Türkçe](https://docs.ultralytics.com/tr) | [Tiếng Việt](https://docs.ultralytics.com/vi) | [العربية](https://docs.ultralytics.com/ar) <br>

<div>
    <a href="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml"><img src="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg" alt="Ultralytics CI"></a>
    <a href="https://zenodo.org/badge/latestdoi/264818686"><img src="https://zenodo.org/badge/264818686.svg" alt="Ultralytics YOLO Citation"></a>
    <a href="https://hub.docker.com/r/ultralytics/ultralytics"><img src="https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker" alt="Ultralytics Docker Pulls"></a>
    <a href="https://discord.com/invite/ultralytics"><img alt="Ultralytics Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue"></a>
    <a href="https://community.ultralytics.com/"><img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue"></a>
    <a href="https://reddit.com/r/ultralytics"><img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue"></a>
    <br>
    <a href="https://console.paperspace.com/github/ultralytics/ultralytics"><img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run Ultralytics on Gradient"></a>
    <a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open Ultralytics In Colab"></a>
    <a href="https://www.kaggle.com/ultralytics/yolov8"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open Ultralytics In Kaggle"></a>
</div>
<br>

[Ultralytics](https://www.ultralytics.com/) [YOLO11](https://github.com/ultralytics/ultralytics)是一款尖端的先进 （SOTA） 型号，它建立在以前 YOLO 版本的成功基础上，并引入了新功能和改进，以进一步提高性能和灵活性。YOLO11 旨在快速、准确且易于使用，使其成为各种对象检测和跟踪、实例分割、图像分类和姿态估计任务的绝佳选择。


<img width="100%" src="https://github.com/user-attachments/assets/a311a4ed-bbf2-43b5-8012-5f183a28a845" alt="YOLO11 performance plots"></a>

<div align="center">
  <a href="https://github.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="2%" alt="Ultralytics GitHub"></a>
  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">
  <a href="https://www.linkedin.com/company/ultralytics/"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="2%" alt="Ultralytics LinkedIn"></a>
  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">
  <a href="https://twitter.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="2%" alt="Ultralytics Twitter"></a>
  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">
  <a href="https://youtube.com/ultralytics?sub_confirmation=1"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="2%" alt="Ultralytics YouTube"></a>
  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">
  <a href="https://www.tiktok.com/@ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="2%" alt="Ultralytics TikTok"></a>
  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">
  <a href="https://ultralytics.com/bilibili"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="2%" alt="Ultralytics BiliBili"></a>
  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">
  <a href="https://discord.com/invite/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="2%" alt="Ultralytics Discord"></a>
</div>
</div>

## <div align="center">Documentation</div>

See below for a quickstart install and usage examples, and see our [Docs](https://docs.ultralytics.com/) for full documentation on training, validation, prediction and deployment.

<details open>
<summary>Install</summary>

Pip install the ultralytics package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).

[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://pepy.tech/project/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)

```bash
pip install ultralytics
```

For alternative installation methods including [Conda](https://anaconda.org/conda-forge/ultralytics), [Docker](https://hub.docker.com/r/ultralytics/ultralytics), and Git, please refer to the [Quickstart Guide](https://docs.ultralytics.com/quickstart/).

[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge)](https://anaconda.org/conda-forge/ultralytics) [![Docker Image Version](https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics)

</details>

<details open>
<summary>Usage</summary>

### CLI

YOLO may be used directly in the Command Line Interface (CLI) with a `yolo` command:

```bash
yolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'
```

`yolo` can be used for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See the YOLO [CLI Docs](https://docs.ultralytics.com/usage/cli/) for examples.

### Python

YOLO may also be used directly in a Python environment, and accepts the same [arguments](https://docs.ultralytics.com/usage/cfg/) as in the CLI example above:

```python
from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")

# Train the model
train_results = model.train(
    data="coco8.yaml",  # path to dataset YAML
    epochs=100,  # number of training epochs
    imgsz=640,  # training image size
    device="cpu",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu
)

# Evaluate model performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model("path/to/image.jpg")
results[0].show()

# Export the model to ONNX format
path = model.export(format="onnx")  # return path to exported model
```

See YOLO [Python Docs](https://docs.ultralytics.com/usage/python/) for more examples.

</details>

## <div align="center">Models</div>

YOLO11 [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/) and [Pose](https://docs.ultralytics.com/tasks/pose/) models pretrained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset are available here, as well as YOLO11 [Classify](https://docs.ultralytics.com/tasks/classify/) models pretrained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) dataset. [Track](https://docs.ultralytics.com/modes/track/) mode is available for all Detect, Segment and Pose models.

<img width="1024" src="https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png" alt="Ultralytics YOLO supported tasks">

All [Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/models) download automatically from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) on first use.

<details open><summary>Detection (COCO)</summary>

See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples with these models trained on [COCO](https://docs.ultralytics.com/datasets/detect/coco/), which include 80 pre-trained classes.

| Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |
| ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |
| [YOLO11n](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt) | 640                   | 39.5                 | 56.1 ± 0.8                     | 1.5 ± 0.0                           | 2.6                | 6.5               |
| [YOLO11s](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt) | 640                   | 47.0                 | 90.0 ± 1.2                     | 2.5 ± 0.0                           | 9.4                | 21.5              |
| [YOLO11m](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt) | 640                   | 51.5                 | 183.2 ± 2.0                    | 4.7 ± 0.1                           | 20.1               | 68.0              |
| [YOLO11l](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt) | 640                   | 53.4                 | 238.6 ± 1.4                    | 6.2 ± 0.1                           | 25.3               | 86.9              |
| [YOLO11x](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt) | 640                   | 54.7                 | 462.8 ± 6.7                    | 11.3 ± 0.2                          | 56.9               | 194.9             |

- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO val2017](https://cocodataset.org/) dataset. <br>Reproduce by `yolo val detect data=coco.yaml device=0`
- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val detect data=coco.yaml batch=1 device=0|cpu`

</details>

<details><summary>Segmentation (COCO)</summary>

See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples with these models trained on [COCO-Seg](https://docs.ultralytics.com/datasets/segment/coco/), which include 80 pre-trained classes.

| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |
| -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |
| [YOLO11n-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt) | 640                   | 38.9                 | 32.0                  | 65.9 ± 1.1                     | 1.8 ± 0.0                           | 2.9                | 10.4              |
| [YOLO11s-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt) | 640                   | 46.6                 | 37.8                  | 117.6 ± 4.9                    | 2.9 ± 0.0                           | 10.1               | 35.5              |
| [YOLO11m-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt) | 640                   | 51.5                 | 41.5                  | 281.6 ± 1.2                    | 6.3 ± 0.1                           | 22.4               | 123.3             |
| [YOLO11l-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt) | 640                   | 53.4                 | 42.9                  | 344.2 ± 3.2                    | 7.8 ± 0.2                           | 27.6               | 142.2             |
| [YOLO11x-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt) | 640                   | 54.7                 | 43.8                  | 664.5 ± 3.2                    | 15.8 ± 0.7                          | 62.1               | 319.0             |

- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO val2017](https://cocodataset.org/) dataset. <br>Reproduce by `yolo val segment data=coco-seg.yaml device=0`
- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val segment data=coco-seg.yaml batch=1 device=0|cpu`

</details>

<details><summary>Classification (ImageNet)</summary>

See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples with these models trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/), which include 1000 pretrained classes.

| Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 640 |
| -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |
| [YOLO11n-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt) | 224                   | 70.0             | 89.4             | 5.0 ± 0.3                      | 1.1 ± 0.0                           | 1.6                | 3.3                      |
| [YOLO11s-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-cls.pt) | 224                   | 75.4             | 92.7             | 7.9 ± 0.2                      | 1.3 ± 0.0                           | 5.5                | 12.1                     |
| [YOLO11m-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt) | 224                   | 77.3             | 93.9             | 17.2 ± 0.4                     | 2.0 ± 0.0                           | 10.4               | 39.3                     |
| [YOLO11l-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt) | 224                   | 78.3             | 94.3             | 23.2 ± 0.3                     | 2.8 ± 0.0                           | 12.9               | 49.4                     |
| [YOLO11x-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt) | 224                   | 79.5             | 94.9             | 41.4 ± 0.9                     | 3.8 ± 0.0                           | 28.4               | 110.4                    |

- **acc** values are model accuracies on the [ImageNet](https://www.image-net.org/) dataset validation set. <br>Reproduce by `yolo val classify data=path/to/ImageNet device=0`
- **Speed** averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`

</details>

<details><summary>Pose (COCO)</summary>

See [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for usage examples with these models trained on [COCO-Pose](https://docs.ultralytics.com/datasets/pose/coco/), which include 1 pre-trained class, person.

| Model                                                                                          | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |
| ---------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |
| [YOLO11n-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt) | 640                   | 50.0                  | 81.0               | 52.4 ± 0.5                     | 1.7 ± 0.0                           | 2.9                | 7.6               |
| [YOLO11s-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-pose.pt) | 640                   | 58.9                  | 86.3               | 90.5 ± 0.6                     | 2.6 ± 0.0                           | 9.9                | 23.2              |
| [YOLO11m-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt) | 640                   | 64.9                  | 89.4               | 187.3 ± 0.8                    | 4.9 ± 0.1                           | 20.9               | 71.7              |
| [YOLO11l-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-pose.pt) | 640                   | 66.1                  | 89.9               | 247.7 ± 1.1                    | 6.4 ± 0.1                           | 26.2               | 90.7              |
| [YOLO11x-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt) | 640                   | 69.5                  | 91.1               | 488.0 ± 13.9                   | 12.1 ± 0.2                          | 58.8               | 203.3             |

- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO Keypoints val2017](https://cocodataset.org/) dataset. <br>Reproduce by `yolo val pose data=coco-pose.yaml device=0`
- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val pose data=coco-pose.yaml batch=1 device=0|cpu`

</details>

<details><summary>OBB (DOTAv1)</summary>

See [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for usage examples with these models trained on [DOTAv1](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/), which include 15 pre-trained classes.

| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>test<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |
| -------------------------------------------------------------------------------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |
| [YOLO11n-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-obb.pt) | 1024                  | 78.4               | 117.6 ± 0.8                    | 4.4 ± 0.0                           | 2.7                | 17.2              |
| [YOLO11s-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-obb.pt) | 1024                  | 79.5               | 219.4 ± 4.0                    | 5.1 ± 0.0                           | 9.7                | 57.5              |
| [YOLO11m-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-obb.pt) | 1024                  | 80.9               | 562.8 ± 2.9                    | 10.1 ± 0.4                          | 20.9               | 183.5             |
| [YOLO11l-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-obb.pt) | 1024                  | 81.0               | 712.5 ± 5.0                    | 13.5 ± 0.6                          | 26.2               | 232.0             |
| [YOLO11x-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-obb.pt) | 1024                  | 81.3               | 1408.6 ± 7.7                   | 28.6 ± 1.0                          | 58.8               | 520.2             |

- **mAP<sup>test</sup>** values are for single-model multiscale on [DOTAv1](https://captain-whu.github.io/DOTA/index.html) dataset. <br>Reproduce by `yolo val obb data=DOTAv1.yaml device=0 split=test` and submit merged results to [DOTA evaluation](https://captain-whu.github.io/DOTA/evaluation.html).
- **Speed** averaged over DOTAv1 val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. <br>Reproduce by `yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu`

</details>

## <div align="center">Integrations</div>

Our key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with [Roboflow](https://roboflow.com/?ref=ultralytics), ClearML, [Comet](https://bit.ly/yolov8-readme-comet), Neural Magic and [OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow.

<br>
<a href="https://www.ultralytics.com/hub" target="_blank">
<img width="100%" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png" alt="Ultralytics active learning integrations"></a>
<br>
<br>

<div align="center">
  <a href="https://roboflow.com/?ref=ultralytics">
    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-roboflow.png" width="10%" alt="Roboflow logo"></a>
  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space">
  <a href="https://clear.ml/">
    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-clearml.png" width="10%" alt="ClearML logo"></a>
  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space">
  <a href="https://bit.ly/yolov8-readme-comet">
    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png" width="10%" alt="Comet ML logo"></a>
  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space">
  <a href="https://bit.ly/yolov5-neuralmagic">
    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png" width="10%" alt="NeuralMagic logo"></a>
</div>

|                                                           Roboflow                                                           |                                                 ClearML ⭐ NEW                                                  |                                                                       Comet ⭐ NEW                                                                        |                                          Neural Magic ⭐ NEW                                           |
| :--------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------: |
| Label and export your custom datasets directly to YOLO11 for training with [Roboflow](https://roboflow.com/?ref=ultralytics) | Automatically track, visualize and even remotely train YOLO11 using [ClearML](https://clear.ml/) (open-source!) | Free forever, [Comet](https://bit.ly/yolov5-readme-comet) lets you save YOLO11 models, resume training, and interactively visualize and debug predictions | Run YOLO11 inference up to 6x faster with [Neural Magic DeepSparse](https://bit.ly/yolov5-neuralmagic) |

## <div align="center">Ultralytics HUB</div>

Experience seamless AI with [Ultralytics HUB](https://www.ultralytics.com/hub) ⭐, the all-in-one solution for data visualization, YOLO11 🚀 model training and deployment, without any coding. Transform images into actionable insights and bring your AI visions to life with ease using our cutting-edge platform and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** now!

<a href="https://www.ultralytics.com/hub" target="_blank">
<img width="100%" src="https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png" alt="Ultralytics HUB preview image"></a>

## <div align="center">Contribute</div>

We love your input! Ultralytics YOLO would not be possible without help from our community. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started, and fill out our [Survey](https://www.ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey) to send us feedback on your experience. Thank you 🙏 to all our contributors!

<!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=990 -->

<a href="https://github.com/ultralytics/ultralytics/graphs/contributors">
<img width="100%" src="https://github.com/ultralytics/assets/raw/main/im/image-contributors.png" alt="Ultralytics open-source contributors"></a>

## <div align="center">License</div>

Ultralytics offers two licensing options to accommodate diverse use cases:

- **AGPL-3.0 License**: This [OSI-approved](https://opensource.org/license) open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the [LICENSE](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) file for more details.
- **Enterprise License**: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out through [Ultralytics Licensing](https://www.ultralytics.com/license).

## <div align="center">Contact</div>

For Ultralytics bug reports and feature requests please visit [GitHub Issues](https://github.com/ultralytics/ultralytics/issues). Become a member of the Ultralytics [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), or [Forums](https://community.ultralytics.com/) for asking questions, sharing projects, learning discussions, or for help with all things Ultralytics!

<br>


## <div align="center">个人项目介绍</div>
<div>
本项目应用于金属表面的缺陷检测
大语言模型下的图像缺陷检测技术的实现

网络模型：YOLOv10 vs UNet                

数据集大小：训练集：3400   验证集：120   测试集：840
数据格式：	200*200像素 || 包含三类金属缺陷
![image](https://github.com/user-attachments/assets/5a5e5563-e95a-4d4f-af06-66ad6d77a6f4)
![image](https://github.com/user-attachments/assets/939823c1-304b-438b-8d53-f8c5c9ea9368)
<div>通过训练出的网络模型进行测试，计算其P、R、class_IOU、mIOU、FPS等性能指标评估模型优劣</div>

YOLO对标同环境配置下的UNet经典模型，横向对比判断模型优劣
![image](https://github.com/user-attachments/assets/c72525c9-b6f8-403f-ab0f-6530b772d02a)
![image](https://github.com/user-attachments/assets/a3d4b4b6-1ec7-452b-ad4b-794928518650)
<div>通过训练出的网络模型进行测试，计算其P、R、class_IOU、mIOU、FPS等性能指标评估模型优劣</div>

YOLO对标同环境配置下的UNet经典模型，横向对比判断模型优劣
![image](https://github.com/user-attachments/assets/92f22d23-6019-4ca1-9cbc-5a40696cf9a6)
![image](https://github.com/user-attachments/assets/9f2551e8-2547-4726-9493-5ec5991ac779)

本模型和UNet经典网络模型最新的性能对比结果：
  #0.5215111364407009 0.7245207416307832 0.5491102116647867 13  36
  #0.6585676173061354 0.8357125632769099 0.7407291079282734 UNet 的 IOU -- mIOU 0.75
  #0.6458035706316883 0.8449470447495435 0.7391515444402353
  #0.6518431611826072 0.8514878966310536 0.7424705997266193   -- mIOU 0.75
  
  #0.753794664940869 0.8682609032773725 0.8331920379862269 YOLO10n 的 IOU -- mIOU 0.82
  #0.7398933240589208 0.8794333157697425 0.8352304672193059
  #0.7402450226560389 0.8848863848684957 0.8365618654226482   -- mIOU 0.82
通过交并混合IOU的测试对比，我们可以发现本项目的平均识别准确率相比经典UNet网络模型提升了0.07
</div>
<div>数据集下载：通过百度网盘分享的文件：trainingr.zip链接：https://pan.baidu.com/s/1gA3fHWneXgnpWwKnvG75NA?pwd=o0f3 提取码：o0f3</div>
钢材表面缺陷检测与分割
第1章 绪论
1.1 研究背景与意义
　　随着工业化进程的加速，钢材作为基础材料在建筑、交通、机械制造等多个领域中扮演者至关重要的角色。钢材的质量直接影响到产品的安全性和可靠性，因此对钢材表面缺陷的检测显得尤为重要。传统的人工检测方法效率低下，受限于检测人员的经验和主观判断，常常导致漏检和误检的问题。此外，随着钢铁生产规模的不断扩大，人工检测难以满足大规模生产的需求。因此，开发高效、准确的自动化检测系统成为了行业内亟待解决的课题。
　　近年来，深度学习技术在图像处理领域取得了显著进展，其强大的特征学习能力使其在缺陷检测和分割任务中展现出良好的应用前景。通过构建基于深度学习的钢材表面缺陷检测与分割模型，可以实现对缺陷区域的精确定位和分类，提高检测的准确性和效率。这不仅能有效减少人工干预，降低生产成本，还能在一定程度上提升产品的整体质量。
　　因此，采用自动化、智能化的检测技术成为了行业发展的必然趋势，基于深度学习的自动化检测技术成为提升钢材表面缺陷检测效率和准确性的重要手段。这项研究不仅具有重要的学术价值，也为实际工业应用提供了新的思路和方法。通过提升钢材表面缺陷的检测效率和准确性，能够有效改善生产流程，降低资源浪费，为钢铁企业的可持续发展做出贡献。

1.2 研究任务
　　本项目使用赛会主办方指定的钢材表面缺陷检测数据集基于YOLOv11设计并实现一种钢材缺陷检测模型，该模型能够高效准确地对钢材表面的缺陷进行像素级分割，实现精确的缺陷识别。
　　本项目主要由数据预处理、模型选择、损失函数设计、优化策略、后处理技术以及模型评估共同构成，确保能够开发出一种既准确又高效的缺陷检测与分割模型。
　　

第2章 基于yolov11网络的算法改进
　　YOLOv11是一种针对目标检测任务的改进型深度学习模型。其核心目标是在保持高精度的同时，进一步提升推理速度和效率。本项目对于YOLOv11的改进主要体现在以下几个方面：
2.1 SPD-Conv空间深度转换卷积改进yolov11
　　这是一个基于YOLOv11架构改进的目标检测模型，在YOLOv11中利用SPD-Conv替换传统的步长卷积和池化层，增强了特征提取能力，提高了物体检测的精度。其精炼的设计和优化的训练流程提升了处理速度，在准确性和性能之间实现了良好的平衡。使用更少的参数，使其在保证准确性的情况下具备更高的计算效率。
(1) SPD-Conv空间深度转换卷积
　　SPD-Conv模块由一个空间-深度（Space-to-Depth, SPD）层和一个非步幅卷积层组成。SPD层将输入特征图按照指定比例下采样，将其空间信息重新排列到通道维度上，以此减少空间分辨率而不丢失细节信息。随后，非步幅卷积层（即步幅为1的卷积）进一步处理这些特征，提取判别性信息，从而实现特征的压缩和增强，同时避免了传统步幅卷积和池化层带来的信息丢失问题。
![image](https://github.com/user-attachments/assets/bc8378b6-10ad-42b2-8807-c9b7229a1452)
图1 SPD-Conv模型图

　　SPD-Conv（空间到深度卷积）的基本原理是用于改进传统卷积神经网络（CNN）中对小物体和低分辨率图像处理的性能。它主要通过以下几个关键步骤实现：
　　1. 替换步长卷积和池化层：SPD-Conv设计用来替代传统CNN架构中的步长卷积层和池化层。步长卷积和池化层在处理低分辨率图像或小物体时会导致细粒度信息的丢失。
　　2. 空间到深度（SPD）层：SPD层的作用是降采样特征图的通道维度，将特征图的空间维度转换成深度维度，通过增加通道数来保留更多信息。这种方式可以避免传统方法中的信息丢失。
　　3.非步长卷积层：在SPD层之后，SPD-Conv使用一个非步长（即步长为1）的卷积层。保持了空间维度，减少了通道数。这种替代方法避免了信息的丢失，并允许网络捕获更精细的特征，从而提高了在复杂任务上的性能。
　　空间到深度（SPD）层是SPD-Conv中的一个关键组件，其作用是将输入特征图的空间块（像素块）重新排列进入深度（通道）维度，以此来增加通道数，同时减少空间分辨率，但不丢失信息。通过这种方式，这一转换允许CNN捕捉和保留在处理小物体和低分辨率图像时经常丢失的精细信息。SPD层后面紧跟的是非步长卷积层，它进一步处理重新排列后的特征图，确保有效特征的提取和使用。通过这种方法，SPD-Conv能够在特征提取阶段保留更丰富的信息，从而提高模型对于小物体和低分辨率图像的识别性能
　　非步长卷积层采用的是步长为1的卷积操作，意味着在卷积过程中，滤波器（或称为卷积核）会在输入特征图上逐像素移动，没有跳过任何像素。这样可以确保在特征图的每个位置都能应用卷积核，最大程度地保留信息，并生成丰富的特征表示。非步长卷积层是紧随空间到深度（SPD）层的一个重要组成部分。在SPD层将输入特征图的空间信息重新映射到深度（通道）维度后，非步长卷积层（即步长为1的卷积层）被用来处理这些重新排列的特征图。由于步长为1，这个卷积层不会导致任何进一步的空间分辨率降低，这允许网络在不损失细节的情况下减少特征图的通道数。这种方法有助于改善特征的表征，特别是在处理小物体或低分辨率图像时，这些场景在传统CNN结构中往往会丢失重要信息。

(2) SPD-Conv模块在YOLOv11的改进
　　如图2所示，SPD-Conv模块基于YOLOv11的改进被分为三个主要部分：
　　1.主干网络（Backbone）：这是特征提取的核心部分，每个SPD和Conv层的组合都替换了原始YOLOv11中的步长卷积层。
　　2.颈部（Neck）：这部分用于进一步处理特征图，以获得不同尺度的特征，从而提高检测不同大小物体的能力。它也包含SPD和Conv层的组合，以优化特征提取。
　　3.头部（Head）：这是决策部分，用于物体检测任务，包括定位和分类。头部保持了YOLO原始架构的设计。
![image](https://github.com/user-attachments/assets/d1f9ba2c-c07e-4511-88c6-a175f52c87f4)
图 2 YOLOv11改进后模型图

　　YOLOv11利用SPD-Conv模块改进Backbone和Neck架构，增强了特征提取能力，提高了物体检测的精度。其精炼的设计和优化的训练流程提升了处理速度，在准确性和性能之间实现了良好的平衡。YOLOv11在数据集上达到了更高的均值平均精度（mAP），使用更少的参数，使其在不妥协准确性的情况下具备更高的计算效率。此外，YOLOv11具有出色的环境适应性，可部署于多种环境中，支持物体检测、实例分割、图像分类、姿态估计和定向物体检测（OBB）等多种计算机视觉任务。
　　
2.2数据增强策略的改进
　　为了提高YOLOv10模型的鲁棒性和泛化能力，改进后的数据增强策略通过模拟多种真实世界的场景变化，增加了图像的多样性，具体实现如下：
　　由于数据集的缺陷样本数量有限，实施有效的数据增强策略可以显著提高模型的泛化能力。本项目采用了多种数据增强技术来丰富数据集，并提高模型对钢材表面缺陷检测的鲁棒性。
(1) 旋转增强
　　对图像应用了不同程度的旋转增强，如图3所示，旋转角度最多可达5度。这模拟了在实际生产环境中可能遇到的不同视角的钢材表面，有助于模型学习到不同方向的缺陷特征。
![image](https://github.com/user-attachments/assets/ad6d608b-dda8-4d52-bf59-4b67623ffec8)
图 3 旋转增强

(2) 平移增强
　　通过平移增强，随机移动图像中的对象，如图4所示。平移范围最多可达图像尺寸的20%。这种增强方式模拟了缺陷在钢材表面不同位置出现的情况，增强了模型对缺陷位置变化的适应能力。
![image](https://github.com/user-attachments/assets/b5e2e6e6-d4ce-4339-b3c6-b0ef5712bba9)
图 4平移增强

(3) 剪切变换
　　剪切变换通过在图像上施加剪切力来模拟不同的视角效果，如图5所示，在数据增强中加入了最多1.0度的剪切变换。即使在视角变化的情况下，也有助于模型更好地理解缺陷的形状和大小。

(4) 缩放增强
　　对图像进行了缩放增强，如图6所示，缩放比例范围从50%到150%。这种增强方式使得模型能够适应不同尺寸的缺陷，提高了模型对缺陷大小变化的识别能力。
![image](https://github.com/user-attachments/assets/3f328317-13f7-41a7-add1-5355b3e6beef)
图 5剪切变换

![image](https://github.com/user-attachments/assets/8b9f2f55-40c6-44c3-998d-04083984c1a9)
图 6缩放增强

（5）Mosaic增强
　　Mosaic增强是一种有效的数据增强技术，通过将四张不同的图像拼接在一起形成一个新的图像，我们以1.0的比例使用mosaic增强，如图7所示。进一步丰富了训练集的多样性，有助于模型学习到更多样化的缺陷特征，并且提高了模型对新场景的泛化能力。
　　

![image](https://github.com/user-attachments/assets/669e5b97-37b6-49bc-a80e-76bfb96c84f7)
图 7 Mosaic增强
　　（6）随机透视变换
　　通过模拟不同的视角和深度效果来增强图像，以0.5的比例应用这种变换。如图8所示，通过改变图像的透视角度使得模型在面对不同视角和深度的缺陷时，仍能保持较高的识别准确率。
![image](https://github.com/user-attachments/assets/fa2c7aab-021e-4978-9e5c-43c87e6df3fc)
图 8 随机透视变换

　（7）HSV色彩空间增强
　　在HSV色彩空间中，对色调(H)、饱和度(S)和明度(V)分别进行了增强，如图9所示，变化范围分别为0.015、0.7和0.4，通过改变这些参数，模型能够适应光照和颜色变化对图像的影响，使得模型能够在不同的光照条件下稳定地识别缺陷。
![image](https://github.com/user-attachments/assets/a2097fba-175c-4d39-80e9-10b5e7eee6be)
图 9 HSV色彩空间增强
（8）遮盖增强
　　通过在图像的一定区域内随机遮盖一部分像素来模拟图像损坏的情况，以0.1的比例使用这种增强，如图10所示，在图像中随机遮盖部分区域，迫使模型在不完全依赖图像局部信息的情况下进行缺陷识别，从而提升模型的鲁棒性，这有助于模型学习到在图像不完整时如何进行缺陷检测。
![image](https://github.com/user-attachments/assets/eaa4a1b7-1881-4121-bb12-3633803097bd)
图 10 遮盖增强

　　采用这些先进的数据增强技术，显著提升了模型针对钢材表面缺陷检测的泛化性和稳健性。这些技术不仅增强了模型在训练数据上的表现，还为模型在实际工业环境中的运用打下了牢固的基础。通过实施这些增强措施，我们得以更加精确地模拟实际生产过程中可能出现的多样化情况，进而确保了模型在现实世界应用中的高度有效性和稳定性。

2.3超参数调优
　　对YOLOv10的超参数进行了调优，目的是为了提升检测精度、提高模型稳定性和泛化能力，同时优化训练效率并满足计算资源的限制。通过合理设置学习率、批量大小、训练轮数等参数，模型能够更好地学习数据特征，增强对钢材表面缺陷的识别精度。选择合适的优化器、动量和权重衰减等超参数，确保模型在训练过程中的稳定性和快速收敛。同时，数据增强和其他参数设置提升了模型的泛化能力，减少过拟合风险。优化数据加载和输入尺寸等策略也提高了训练效率，确保在有限资源下实现良好性能。

